# Adversarial attack на датасете MNIST
Генерация шума, наложение которого на исходное изображение заставит классификатор (LeNet) присваивать неправильную метку класса.  
Задача решена двумя разными способами:
 - В первом варианте мы изменяем исходную картинку, пытаясь подтянуть её ко второму по вероятности классу, то есть к наиболее похожему изображению. Разница между исходным изображением и изменённым и будет искомым фильтром.
 - Во втором варианте изначально добавляется пустой фильтр - изображение, инициированное нулями. Все дальнейшие изменения вносятся только в этот фильтр, не трогая изначальное изображение.  
   
Оба варианта показали примерно одинаковый результат, однако в первом случае зачастую проще получить более "прозрачный" фильтр.

## Стек используемых инструментов:
 - Pytorch
 - Numpy
 - PIL
 - Matplotlib